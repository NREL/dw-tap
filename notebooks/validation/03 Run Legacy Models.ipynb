{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66d3dde6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run Legacy Models (Perera)\n",
    "\n",
    "Caleb Phillips (caleb.phillips@nrel.gov) and Dmitry Duplyakin (dmitry.duplyakin@nrel.gov)\n",
    "\n",
    "The purpose of this notebook is to run the Legacy \"LOM\"s which form a baseline for more complex models. These include:\n",
    "\n",
    " - Vanilla Perera: The original Perera with infinite length obstacles\n",
    " - Perera 2: A version of Perera proposed in the WaSP paper with finite length obstacles\n",
    " - Perera 3: A version of Perera proposed in the WaSP paper with asymmetric finite length obstacles\n",
    " \n",
    "In practice, all three models are quite similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532eb696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from dw_tap.lom import run_lom\n",
    "import os\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from dw_tap.data_processing import _LatLon_To_XY, filter_obstacles\n",
    "\n",
    "from pyproj import Transformer\n",
    "from shapely.geometry import LineString, Polygon, MultiPolygon, Point, MultiPoint, shape\n",
    "from shapely.ops import split, nearest_points\n",
    "import fiona\n",
    "import perera\n",
    "import pickle\n",
    "import common\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "obstacle_data_dir = \"01 Bergey Turbine Data/3dbuildings_geojson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaa688a-d76f-46e3-ba77-3106e048f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.read_csv(\"01 Bergey Turbine Data/bergey_sites.csv\")\n",
    "index.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5d8b2e-d9ea-4996-a15f-72bd5abd11cc",
   "metadata": {},
   "source": [
    "### Select which sites need to be processed and wind data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56e211c-0da0-435e-9124-63caa9107d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small test with several sites\n",
    "#selected = [\"t133\", \"t135\"]\n",
    "\n",
    "# Process all sites:\n",
    "selected = index[\"APRS ID\"].tolist()\n",
    "\n",
    "# Remove 2 sites that currently don't have obstacle descriptions with the heights based on lidar data\n",
    "selected = [x for x in selected if not(x in [\"t007\", \"t074\"])]\n",
    "print(selected)\n",
    "\n",
    "# One or more of: [\"wtk_bc\", wtk\", \"wtk_led_2018\", \"wtk_led_2019\"]\n",
    "wind_sources = [\"wtk\",\"wtk_bc\",\"wtk_led_2018\",\"wtk_led_2019\",\"wtk_led_bc\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec40ab5-38e0-49ee-a549-6c8578f5b8cc",
   "metadata": {},
   "source": [
    "### Load wind data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4da36dd-aa59-43a4-9c75-38995b2d9a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "atmospheric_inputs = {}\n",
    "\n",
    "for wind_source in tqdm(wind_sources):\n",
    "\n",
    "    dfs_by_tid = {}\n",
    "    \n",
    "    if wind_source == \"wtk\":\n",
    "\n",
    "        wtk_df = pd.read_csv(\"01 Bergey Turbine Data/wtk.csv.bz2\")\n",
    "        # Create dict with dataframes that correspond to selected tid's\n",
    "        for tid in selected:\n",
    "            dfs_by_tid[tid] = wtk_df[wtk_df[\"tid\"] == tid].reset_index(drop=True)\n",
    "            #display(dfs_by_tid[tid].head(3))\n",
    "\n",
    "    elif wind_source == \"wtk_led_2018\":\n",
    "\n",
    "        wtk_led_2018 = pd.read_csv(\"01 Bergey Turbine Data/wtk_led_2018.csv.bz2\")\n",
    "\n",
    "        # Create dict with dataframes that correspond to selected tid's\n",
    "        for tid in selected:\n",
    "            dfs_by_tid[tid] = wtk_led_2018[wtk_led_2018[\"tid\"] == tid].copy().reset_index(drop=True)\n",
    "            dfs_by_tid[tid][\"datetime\"] = dfs_by_tid[tid][\"packet_date\"]\n",
    "\n",
    "    elif wind_source == \"wtk_led_2019\":\n",
    "\n",
    "        wtk_led_2019 = pd.read_csv(\"01 Bergey Turbine Data/wtk_led_2019.csv.bz2\")\n",
    "\n",
    "        # Create dict with dataframes that correspond to selected tid's\n",
    "        for tid in selected:\n",
    "            dfs_by_tid[tid] = wtk_led_2019[wtk_led_2019[\"tid\"] == tid].copy().reset_index(drop=True)\n",
    "            dfs_by_tid[tid][\"datetime\"] = dfs_by_tid[tid][\"packet_date\"]\n",
    "\n",
    "    elif wind_source == \"wtk_bc\":\n",
    "        wtk_bc_df = pd.read_csv(\"02 Bias Correction/wtk_bc.csv.bz2\")\n",
    "        \n",
    "        # Create dict with dataframes that correspond to selected tid's\n",
    "        for tid in selected:\n",
    "            dfs_by_tid[tid] = wtk_bc_df[wtk_bc_df[\"tid\"] == tid].reset_index(drop=True)\n",
    "            \n",
    "            # Actually use bias corrected wind speeds for further steps (overwrite original ws)\n",
    "            dfs_by_tid[tid][\"ws\"] = dfs_by_tid[tid][\"ws_bc\"]\n",
    "        \n",
    "    elif wind_source == \"wtk_led_bc\":\n",
    "        wtk_led_bc_df = pd.read_csv(\"02 Bias Correction/wtk_led_bc.csv.bz2\")\n",
    "        \n",
    "        # Create dict with dataframes that correspond to selected tid's\n",
    "        for tid in selected:\n",
    "            dfs_by_tid[tid] = wtk_led_bc_df[wtk_led_bc_df[\"tid\"] == tid].reset_index(drop=True)\n",
    "            \n",
    "            # Actually use bias corrected wind speeds for further steps (overwrite original ws)\n",
    "            dfs_by_tid[tid][\"ws\"] = dfs_by_tid[tid][\"ws_bc\"]\n",
    "        \n",
    "    else:\n",
    "        print(\"Unsupported wind_source selected:\", wind_source)\n",
    "        continue\n",
    "        \n",
    "    atmospheric_inputs[wind_source] = dfs_by_tid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14878556-c3d2-460d-b15d-1c68ba78b2b5",
   "metadata": {},
   "source": [
    "### Load obstacle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab492c6-2512-4efd-95bd-e038834a5ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_with_tall_blgs = [] \n",
    "\n",
    "obstacle_inputs = {}\n",
    "for tid in selected:\n",
    "    \n",
    "    index_row = index[index[\"APRS ID\"] == tid].iloc[0]\n",
    "    z_turbine = index_row[\"Hub Height (m)\"]\n",
    "    \n",
    "    obstacle_data_dir = \"01 Bergey Turbine Data/3dbuildings_geojson\"\n",
    "    obstacle_data_file = \"%s/%sv2.json\" % (obstacle_data_dir, tid)\n",
    "    \n",
    "    if os.path.exists(obstacle_data_file):\n",
    "        #print(\"BEFORE filtering (%s):\" % obstacle_data_file)\n",
    "        #display(gpd.read_file(obstacle_data_file))\n",
    "        \n",
    "        obstacle_df = filter_obstacles(gpd.read_file(obstacle_data_file), \n",
    "                                       include_trees=True, \n",
    "                                       turbine_height_for_checking=z_turbine)\n",
    "        obstacle_df[\"tid\"] = tid\n",
    "        obstacle_inputs[tid] = obstacle_df\n",
    "        \n",
    "        #print(\"AFTER filtering (%s):\" % obstacle_data_file)\n",
    "        #display(obstacle_df)\n",
    "    else:\n",
    "        print(\"Can't access: %s. Skipping\" % obstacle_data_file)\n",
    "\n",
    "all_obstacle_inputs = pd.concat(obstacle_inputs.values())\n",
    "display(all_obstacle_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae11b87a-cc87-4fcb-9b18-3d1fc44d6a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined and filtered obstacles dataframe into a file\n",
    "dest_file = \"%s/all_obstacles.json\" % (obstacle_data_dir)\n",
    "all_obstacle_inputs.to_file(dest_file, driver=\"GeoJSON\", index=False)\n",
    "\n",
    "dest_file = \"%s/all_obstacles_epsg3740.json\" % (obstacle_data_dir)\n",
    "all_obstacle_inputs.to_crs(3740).to_file(dest_file, driver=\"GeoJSON\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b013cbc2-1919-4691-a2fe-84cdc275d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick vis:\n",
    "for tid, obstacle_df in obstacle_inputs.items():\n",
    "    obstacle_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad491749-7b9b-4488-8f04-f021834fccf2",
   "metadata": {},
   "source": [
    "### Calculate and Save Perera Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5481ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer.from_crs(\"epsg:4326\", \"epsg:3740\")\n",
    "buildings = fiona.open(\"%s/all_obstacles_epsg3740.json\" % (obstacle_data_dir))\n",
    "\n",
    "features = {}\n",
    "for tid in tqdm(selected):\n",
    "    row = index[index[\"APRS ID\"] == tid].iloc[0]\n",
    "    lat = row[\"Latitude\"]\n",
    "    lon = row[\"Longitude\"]\n",
    "    lat,lon = transformer.transform(lat,lon)\n",
    "    point = Point(lat,lon)\n",
    "    features[tid] = perera.calculate_perera_features(point,buildings)\n",
    "    \n",
    "pickle.dump( features, open( \"%s/perera_features.p\" % (obstacle_data_dir,), \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1774d5ff",
   "metadata": {},
   "source": [
    "### Run Perera Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e38ca-2668-4df0-8bcc-c8781b696b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This flag allows overwriting previously saved files with results if they are found in the specified directory dest_dir \n",
    "overwrite = False\n",
    "\n",
    "# Will be used in the filenames\n",
    "site_type = \"bergey\"\n",
    "\n",
    "# Will be used in the filenames\n",
    "model_type = \"perera\"\n",
    "\n",
    "dest_dir = \"03 Model Outputs\"\n",
    "if not os.path.exists(dest_dir):\n",
    "    os.makedirs(dest_dir)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0eb17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(atmospheric_inputs[\"wtk\"][\"t024\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614b7303",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pickle.load(open(\"%s/perera_features.p\" % (obstacle_data_dir,),\"rb\"))\n",
    "\n",
    "for tid in tqdm(selected):\n",
    "    \n",
    "    for wind_source in wind_sources:\n",
    "        \n",
    "        # skip those combinations that don't have data\n",
    "        if len(atmospheric_inputs[wind_source][tid]) == 0:\n",
    "            continue\n",
    "        \n",
    "        dest_filename = \"%s/%s_%s_%s_%s.csv.bz2\" % (dest_dir, site_type, model_type, tid, wind_source)\n",
    "        if (not overwrite) and (os.path.exists(dest_filename)):\n",
    "            print(\"Found previously saved %s); overwrite flag is off. Skipping to next config.\" % (dest_filename))\n",
    "            continue\n",
    "           \n",
    "        atmospheric_inputs[wind_source][tid][\"sector\"] = common.sectorize(atmospheric_inputs[wind_source][tid][\"ws\"])\n",
    "        \n",
    "        atmospheric_inputs[wind_source][tid][\"ws-adjusted\"] = \\\n",
    "            atmospheric_inputs[wind_source][tid][[\"tid\",\"sector\",\"ws\"]].\\\n",
    "            apply(perera.perera,axis=1,args=(features,))\n",
    "        atmospheric_inputs[wind_source][tid][\"ws-adjusted-2\"] = \\\n",
    "            atmospheric_inputs[wind_source][tid][[\"tid\",\"sector\",\"ws\"]].\\\n",
    "            apply(perera.perera2,axis=1,args=(features,))\n",
    "        atmospheric_inputs[wind_source][tid][\"ws-adjusted-3\"] = \\\n",
    "            atmospheric_inputs[wind_source][tid][[\"tid\",\"sector\",\"ws\"]].\\\n",
    "            apply(perera.perera3,axis=1,args=(features,))\n",
    "        \n",
    "        atmospheric_inputs[wind_source][tid].to_csv(dest_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71835cca-b571-4e78-9515-7a3f60fad83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick vis of data in produced files\n",
    "\n",
    "for f in glob.iglob(\"%s/*perera*\" % dest_dir):\n",
    "    df = pd.read_csv(f)\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(2.5,2.5)\n",
    "    sns.scatterplot(x=df[\"ws\"], \\\n",
    "                    y=df[\"ws-adjusted\"], alpha=0.2).set(title=os.path.basename(f));\n",
    "    #plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
